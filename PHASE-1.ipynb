{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e76115f",
   "metadata": {},
   "source": [
    "This notebook builds a Retrieval-Augmented Generation (RAG) chatbot for the Lab of Future website. It follows these steps:\n",
    "1. Web scraping with BeautifulSoup and requests\n",
    "2. Text chunking \n",
    "3. Embedding generation with Sentence Transformers\n",
    "4. Knowledge base setup with FAISS\n",
    "5. Agent creation with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b38e40",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9299cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (0.3.17)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.3.24)\n",
      "Requirement already satisfied: langchain-text-splitters in /opt/anaconda3/lib/python3.12/site-packages (0.3.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.60)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.10.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (1.79.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.7.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 requests pandas tqdm sentence-transformers faiss-cpu langchain langchain-openai langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492e325f",
   "metadata": {},
   "source": [
    "## 2. Web Scraping - Collecting Data from Lab of Future Website\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbf23d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://www.laboffuture.com/\n",
      "Crawling: https://www.laboffuture.com/#myCarousel1721988882528\n",
      "Crawling: https://www.laboffuture.com/summer-camp-2025\n",
      "Crawling: https://www.laboffuture.com/summer-camp-2025-form\n",
      "Crawling: https://www.laboffuture.com/summer-camp-2025#myCarousel1746438217777\n",
      "Crawling: https://www.laboffuture.com/summer-camp-2025#nav_tabs_content_1746549997829_379\n",
      "Crawling: https://www.laboffuture.com/summer-camp-2025#nav_tabs_content_1746549997829_380\n",
      "Crawling: https://www.laboffuture.com/contactus\n",
      "Crawling: https://www.laboffuture.com/#table_of_content_heading_1725885336557_77\n",
      "Crawling: https://www.laboffuture.com/#table_of_content_heading_1725885336557_78\n",
      "Crawling: https://www.laboffuture.com/celestial-voyages\n",
      "Crawling: https://www.laboffuture.com/celestial-voyages#myCarousel1726032948445\n",
      "Crawling: https://www.laboffuture.com/celestial-voyages#nav_tabs_content_1723674780460_850\n",
      "Crawling: https://www.laboffuture.com/celestial-voyages#nav_tabs_content_1723674780460_851\n",
      "Crawling: https://www.laboffuture.com/celestial-voyages#nav_tabs_content_1723674780460_852\n",
      "Crawling: https://www.laboffuture.com/celestial-voyages#nav_tabs_content_1723674780460_853\n",
      "Crawling: https://www.laboffuture.com/celestial-voyages#table_of_content_heading_1723406338329_2316\n",
      "Crawling: https://www.laboffuture.com/celestial-voyages#table_of_content_heading_1723406338329_2317\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics#myCarousel1726563399670\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics#myCarousel1733574797040\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics#nav_tabs_content_1727956997858_273\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics#nav_tabs_content_1727956997858_274\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics#nav_tabs_content_1727956997858_275\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics#myCarousel1726548144589\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics#table_of_content_heading_1725542221016_58\n",
      "Crawling: https://www.laboffuture.com/galactic-mechanics#table_of_content_heading_1725542221016_59\n",
      "Crawling: https://www.laboffuture.com/iot-illuminations\n",
      "Crawling: https://www.laboffuture.com/iot-illuminations#nav_tabs_content_1723674780460_852\n",
      "Crawling: https://www.laboffuture.com/iot-illuminations#nav_tabs_content_1723674780460_853\n",
      "Crawling: https://www.laboffuture.com/iot-illuminations#myCarousel1722099725629\n",
      "Crawling: https://www.laboffuture.com/iot-illuminations#table_of_content_heading_1723406338329_2316\n",
      "Crawling: https://www.laboffuture.com/iot-illuminations#table_of_content_heading_1723406338329_2318\n",
      "Crawling: https://www.laboffuture.com/iot-illuminations#table_of_content_heading_1723406338329_2319\n",
      "Crawling: https://www.laboffuture.com/iot-illuminations#table_of_content_heading_1723406338329_2317\n",
      "Crawling: https://www.laboffuture.com/robotic-realms\n",
      "Crawling: https://www.laboffuture.com/robotic-realms#nav_tabs_content_1723674780460_850\n",
      "Crawling: https://www.laboffuture.com/robotic-realms#nav_tabs_content_1723674780460_851\n",
      "Crawling: https://www.laboffuture.com/robotic-realms#nav_tabs_content_1723674780460_852\n",
      "Crawling: https://www.laboffuture.com/robotic-realms#nav_tabs_content_1723674780460_853\n",
      "Crawling: https://www.laboffuture.com/robotic-realms#table_of_content_heading_1725488083423_59\n",
      "Crawling: https://www.laboffuture.com/robotic-realms#table_of_content_heading_1725488083423_60\n",
      "Crawling: https://www.laboffuture.com/artificialintelligence\n",
      "Crawling: https://www.laboffuture.com/artificialintelligence#nav_tabs_content_1728027979482_398\n",
      "Crawling: https://www.laboffuture.com/artificialintelligence#nav_tabs_content_1728027979486_399\n",
      "Crawling: https://www.laboffuture.com/artificialintelligence#nav_tabs_content_1728027979486_401\n",
      "Crawling: https://www.laboffuture.com/artificialintelligence#table_of_content_heading_1725481555936_146\n",
      "Crawling: https://www.laboffuture.com/artificialintelligence#table_of_content_heading_1725481555936_147\n",
      "Crawling: https://www.laboffuture.com/airborne-wonders-1\n",
      "Crawling: https://www.laboffuture.com/airborne-wonders-1#nav_tabs_content_1736750188753_805\n",
      "Crawling complete. Data saved to laboffuture_content.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize\n",
    "root_url = \"https://www.laboffuture.com/\"\n",
    "visited = set()\n",
    "data = []\n",
    "\n",
    "def clean_text(soup):\n",
    "    # Remove script, style, nav, footer, header tags\n",
    "    for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'noscript', 'form', 'iframe']):\n",
    "        tag.decompose()\n",
    "    # Extract visible text\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    # Normalize whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def crawl(url, max_pages=50):\n",
    "    if len(visited) >= max_pages:\n",
    "        return\n",
    "    if url in visited:\n",
    "        return\n",
    "    print(f\"Crawling: {url}\")\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Clean page text\n",
    "        page_text = clean_text(soup)\n",
    "        data.append({'url': url, 'content': page_text})\n",
    "\n",
    "        # Find all internal links and crawl them recursively\n",
    "        domain = urlparse(root_url).netloc\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            joined_link = urljoin(url, href)\n",
    "            parsed_link = urlparse(joined_link)\n",
    "            # Only crawl same domain, skip mailto:, tel:, javascript: etc\n",
    "            if parsed_link.netloc == domain and parsed_link.scheme in ('http', 'https'):\n",
    "                if joined_link not in visited:\n",
    "                    crawl(joined_link, max_pages)\n",
    "        # Sleep a bit to be polite\n",
    "        time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "# Start crawling\n",
    "crawl(root_url, max_pages=50)\n",
    "\n",
    "# Save data as CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('laboffuture_content.csv', index=False)\n",
    "\n",
    "print(\"Crawling complete. Data saved to laboffuture_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1623613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 18741\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the chunker (adjust chunk_size and chunk_overlap as needed)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "chunk_texts = []\n",
    "chunk_urls = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Split the page content into chunks\n",
    "    chunks = text_splitter.split_text(row['content'])\n",
    "    # Append chunks and corresponding URL\n",
    "    chunk_texts.extend(chunks)\n",
    "    chunk_urls.extend([row['url']] * len(chunks))\n",
    "\n",
    "print(f\"Total chunks created: {len(chunk_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41fcbfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages scraped: 50\n",
      "Total chunks created: 18741\n",
      "Generating embeddings for chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89b8ed7eb8a4cc0a3b0b5f799329294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (18741, 384)\n",
      "Chunk metadata and embeddings saved!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install langchain.text_splitter if needed\n",
    "# !pip install langchain\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your scraped data is in df with columns ['url', 'content']\n",
    "print(f\"Total pages scraped: {len(df)}\")\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "chunk_texts = []\n",
    "chunk_urls = []\n",
    "\n",
    "# Chunk each page's content and keep URL mapping\n",
    "for i, row in df.iterrows():\n",
    "    chunks = text_splitter.split_text(row['content'])\n",
    "    chunk_texts.extend(chunks)\n",
    "    chunk_urls.extend([row['url']] * len(chunks))\n",
    "\n",
    "print(f\"Total chunks created: {len(chunk_texts)}\")\n",
    "\n",
    "# Step 2: Generate embeddings for chunks\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Generating embeddings for chunks...\")\n",
    "embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Step 3: Save chunk metadata and embeddings separately\n",
    "chunked_df = pd.DataFrame({\n",
    "    'chunk_text': chunk_texts,\n",
    "    'url': chunk_urls\n",
    "})\n",
    "\n",
    "chunked_df.to_csv('laboffuture_content_chunks_metadata.csv', index=False)\n",
    "np.save('laboffuture_embeddings.npy', embeddings)\n",
    "\n",
    "print(\"Chunk metadata and embeddings saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992c593",
   "metadata": {},
   "source": [
    "## 3. Text Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53054b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 21069\n",
      "Chunks saved to laboffuture_content_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('laboffuture_content.csv')\n",
    "\n",
    "# Parameters for chunking\n",
    "chunk_size = 500  # number of characters per chunk\n",
    "overlap = 100     # number of characters to overlap\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # move forward with overlap\n",
    "    return chunks\n",
    "\n",
    "# Prepare a list to hold all chunks with their source URL\n",
    "chunked_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['url']\n",
    "    content = str(row['content'])  # ensure text format\n",
    "    if len(content) > 0:\n",
    "        chunks = chunk_text(content, chunk_size, overlap)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_data.append({\n",
    "                'url': url,\n",
    "                'chunk_index': i,\n",
    "                'chunk_text': chunk\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame for further processing or saving\n",
    "chunked_df = pd.DataFrame(chunked_data)\n",
    "\n",
    "# Save chunked data to CSV\n",
    "chunked_df.to_csv('laboffuture_content_chunks.csv', index=False)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunked_df)}\")\n",
    "print(\"Chunks saved to laboffuture_content_chunks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3631112",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Generate Embeddings with Sentence Transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a06de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 21069 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a624e8453e547b9a516d5a9e951d8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load chunked data\n",
    "chunked_df = pd.read_csv('laboffuture_content_chunks.csv')\n",
    "\n",
    "# Initialize embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # fast and good for many tasks\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "texts = chunked_df['chunk_text'].tolist()\n",
    "print(f\"Generating embeddings for {len(texts)} chunks...\")\n",
    "\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Save embeddings with chunk metadata\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to list for storage\n",
    "embedding_list = embeddings.tolist()\n",
    "\n",
    "chunked_df['embedding'] = embedding_list\n",
    "\n",
    "# Save to a new CSV or preferably to a binary file (like npz) because embeddings are large\n",
    "chunked_df.to_pickle('laboffuture_content_chunks_with_embeddings.pkl')\n",
    "\n",
    "print(\"Embeddings generated and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169569d5",
   "metadata": {},
   "source": [
    "## 5. Create FAISS Knowledge Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "299b0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors indexed: 21069\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the chunk data with embeddings\n",
    "chunked_df = pd.read_pickle('laboffuture_content_chunks_with_embeddings.pkl')\n",
    "\n",
    "# Extract embeddings as a NumPy array\n",
    "embedding_dim = len(chunked_df['embedding'][0])\n",
    "embeddings = np.array(chunked_df['embedding'].tolist()).astype('float32')\n",
    "\n",
    "# Initialize FAISS index - use IndexFlatL2 for simplicity (L2 distance)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"Number of vectors indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b14367c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to laboffuture_faiss.index\n"
     ]
    }
   ],
   "source": [
    "faiss.write_index(index, 'laboffuture_faiss.index')\n",
    "print(\"FAISS index saved to laboffuture_faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1406ddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches:\n",
      "Rank 1, Distance: 0.5124529600143433\n",
      "URL: https://www.laboffuture.com/\n",
      "Text snippet: Home | Lab Of Future Previous Next × Building a community of Future Scientists, today. Get involved Building a Community of Future Scientists, today. Get involved ​​Who are we ​​​​What we do ​​ Who ar...\n",
      "\n",
      "Rank 2, Distance: 0.5124529600143433\n",
      "URL: https://www.laboffuture.com/#myCarousel1721988882528\n",
      "Text snippet: Home | Lab Of Future Previous Next × Building a community of Future Scientists, today. Get involved Building a Community of Future Scientists, today. Get involved ​​Who are we ​​​​What we do ​​ Who ar...\n",
      "\n",
      "Rank 3, Distance: 0.5124529600143433\n",
      "URL: https://www.laboffuture.com/#table_of_content_heading_1725885336557_77\n",
      "Text snippet: Home | Lab Of Future Previous Next × Building a community of Future Scientists, today. Get involved Building a Community of Future Scientists, today. Get involved ​​Who are we ​​​​What we do ​​ Who ar...\n",
      "\n",
      "Rank 4, Distance: 0.5124529600143433\n",
      "URL: https://www.laboffuture.com/#table_of_content_heading_1725885336557_78\n",
      "Text snippet: Home | Lab Of Future Previous Next × Building a community of Future Scientists, today. Get involved Building a Community of Future Scientists, today. Get involved ​​Who are we ​​​​What we do ​​ Who ar...\n",
      "\n",
      "Rank 5, Distance: 0.6296427249908447\n",
      "URL: https://www.laboffuture.com/\n",
      "Text snippet: nd curiosity about space and technology. 22 Cutting-edge Programs 20,000+ ​Students Benefitted 8 International Awards 3 Missions currently active Don't just take our word for it “Lab of Future is an i...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample query text\n",
    "query_text = \"Learn about Lab of Future's space programs\"\n",
    "\n",
    "# Embed query using same model\n",
    "query_embedding = model.encode([query_text]).astype('float32')\n",
    "\n",
    "# Search top 5 similar chunks\n",
    "k = 5\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(\"Top matches:\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Rank {i+1}, Distance: {distances[0][i]}\")\n",
    "    print(f\"URL: {chunked_df.iloc[idx]['url']}\")\n",
    "    print(f\"Text snippet: {chunked_df.iloc[idx]['chunk_text'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10ec5b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "URL: https://www.laboffuture.com/iot-illuminations\n",
      "Distance: 1.0364\n",
      "Text: transforming education. They bring a wealth of knowledge and hands-on experience in their respective fields, including: Robotics Engineers Mechanical Engineers Electronics Engineers Computer Science Engineers Aerospace Engineers Mechatronics Engineers Astro-physicists ​...\n",
      "\n",
      "Result 2:\n",
      "URL: https://www.laboffuture.com/iot-illuminations#nav_tabs_content_1723674780460_852\n",
      "Distance: 1.0364\n",
      "Text: transforming education. They bring a wealth of knowledge and hands-on experience in their respective fields, including: Robotics Engineers Mechanical Engineers Electronics Engineers Computer Science Engineers Aerospace Engineers Mechatronics Engineers Astro-physicists ​...\n",
      "\n",
      "Result 3:\n",
      "URL: https://www.laboffuture.com/iot-illuminations#nav_tabs_content_1723674780460_853\n",
      "Distance: 1.0364\n",
      "Text: transforming education. They bring a wealth of knowledge and hands-on experience in their respective fields, including: Robotics Engineers Mechanical Engineers Electronics Engineers Computer Science Engineers Aerospace Engineers Mechatronics Engineers Astro-physicists ​...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class KnowledgeBase:\n",
    "    def __init__(self, \n",
    "                 faiss_index_path: str, \n",
    "                 chunks_pickle_path: str,\n",
    "                 embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(faiss_index_path)\n",
    "        # Load chunk metadata with embeddings\n",
    "        self.chunks_df = pd.read_pickle(chunks_pickle_path)\n",
    "        # Load embedding model\n",
    "        self.model = SentenceTransformer(embedding_model_name)\n",
    "        # Embedding dimension inferred from model\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        \n",
    "    def query(self, query_text: str, top_k: int = 5):\n",
    "        # Embed the query text\n",
    "        query_embedding = self.model.encode([query_text]).astype('float32')\n",
    "        # Search in FAISS\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Collect results\n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            chunk_data = self.chunks_df.iloc[idx]\n",
    "            results.append({\n",
    "                'url': chunk_data['url'],\n",
    "                'chunk_index': chunk_data['chunk_index'],\n",
    "                'chunk_text': chunk_data['chunk_text'],\n",
    "                'distance': float(dist)\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    kb = KnowledgeBase('laboffuture_faiss.index', 'laboffuture_content_chunks_with_embeddings.pkl')\n",
    "    user_query = \"Tell me about your educational programs\"\n",
    "    answers = kb.query(user_query, top_k=3)\n",
    "    for i, ans in enumerate(answers, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"URL: {ans['url']}\")\n",
    "        print(f\"Distance: {ans['distance']:.4f}\")\n",
    "        print(f\"Text: {ans['chunk_text'][:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d894c714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built with 18741 vectors\n",
      "Top 3 matching URLs:\n",
      "https://www.laboffuture.com/\n",
      "https://www.laboffuture.com/#myCarousel1721988882528\n",
      "https://www.laboffuture.com/#table_of_content_heading_1725885336557_77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load metadata and embeddings\n",
    "chunked_df = pd.read_csv('laboffuture_content_chunks_metadata.csv')\n",
    "embeddings = np.load('laboffuture_embeddings.npy')\n",
    "\n",
    "urls = chunked_df['url'].tolist()\n",
    "\n",
    "# Build FAISS index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} vectors\")\n",
    "\n",
    "# Example similarity search (search top 3 matches for first chunk)\n",
    "D, I = index.search(np.array([embeddings[0]]), k=3)\n",
    "\n",
    "print(\"Top 3 matching URLs:\")\n",
    "for idx in I[0]:\n",
    "    print(urls[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ec9e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: 'solar system exploration'\n",
      "URL: https://www.laboffuture.com/celestial-voyages, Distance: 0.7719335556030273\n",
      "URL: https://www.laboffuture.com/celestial-voyages, Distance: 0.7719335556030273\n",
      "URL: https://www.laboffuture.com/celestial-voyages, Distance: 0.7719335556030273\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "# Load chunk metadata, embeddings, and initialize model and index (do this once)\n",
    "def initialize_search_system():\n",
    "    chunked_df = pd.read_csv('laboffuture_content_chunks_metadata.csv')\n",
    "    embeddings = np.load('laboffuture_embeddings.npy')\n",
    "    chunk_texts = chunked_df['chunk_text'].tolist()\n",
    "    urls = chunked_df['url'].tolist()\n",
    "    \n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    return model, index, chunk_texts, urls\n",
    "\n",
    "# Query function\n",
    "def search_similar_chunks(query_text, model, index, chunk_texts, urls, top_k=3):\n",
    "    query_embedding = model.encode([query_text])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\n",
    "            'chunk_text': chunk_texts[idx],\n",
    "            'url': urls[idx],\n",
    "            'distance': dist\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Initialize once\n",
    "model, index, chunk_texts, urls = initialize_search_system()\n",
    "\n",
    "# Example query\n",
    "query = \"solar system exploration\"\n",
    "results = search_similar_chunks(query, model, index, chunk_texts, urls, top_k=3)\n",
    "\n",
    "print(f\"Results for query: '{query}'\")\n",
    "for r in results:\n",
    "    print(f\"URL: {r['url']}, Distance: {r['distance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61c4cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, contexts):\n",
    "    prompt = \"Answer the question based on the context below. If not found, say 'I don't know'.\\n\\n\"\n",
    "    prompt += \"Context:\\n\"\n",
    "    for i, context in enumerate(contexts):\n",
    "        prompt += f\"{i+1}. {context}\\n\"\n",
    "    prompt += f\"\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_tokens=150,\n",
    "    temperature=0,\n",
    "    )\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6b2b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual Exploration of the Solar System with AR & VR\n",
      "Answer: None\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"solar system exploration\"\n",
    "results = search_similar_chunks(query, model, index, chunk_texts, urls, top_k=3)\n",
    "contexts = [res['chunk_text'] for res in results]\n",
    "answer = generate_answer(query, contexts)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c264cae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.93.1-py2.py3-none-any.whl.metadata (35 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.40.1)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-aiplatform)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-cloud-aiplatform) (5.29.4)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Collecting google-cloud-storage<3.0.0,>=1.32.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_bigquery-3.33.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0,>=1.3.3 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_resource_manager-1.14.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting shapely<3.0.0 (from google-cloud-aiplatform)\n",
      "  Downloading shapely-2.1.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting google-genai<2.0.0,>=1.0.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_genai-1.16.1-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: pydantic<3 in /opt/anaconda3/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.10.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.12/site-packages (from google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/anaconda3/lib/python3.12/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading grpcio_status-1.72.0rc1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform)\n",
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting packaging>=14.3 (from google-cloud-aiplatform)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Collecting grpc-google-iam-v1<1.0.0,>=0.14.0 (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform)\n",
      "  Downloading grpc_google_iam_v1-0.14.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform)\n",
      "  Downloading google_crc32c-1.7.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (15.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3->google-cloud-aiplatform) (2.27.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/anaconda3/lib/python3.12/site-packages (from shapely<3.0.0->google-cloud-aiplatform) (1.26.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.3.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-aiplatform)\n",
      "  Downloading protobuf-6.31.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading grpcio-1.72.0rc1-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.2.3)\n",
      "Downloading google_cloud_aiplatform-1.93.1-py2.py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_cloud_bigquery-3.33.0-py3-none-any.whl (253 kB)\n",
      "Downloading google_cloud_resource_manager-1.14.2-py3-none-any.whl (394 kB)\n",
      "Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
      "Downloading google_genai-1.16.1-py3-none-any.whl (196 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading shapely-2.1.1-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.7.1-cp312-cp312-macosx_12_0_arm64.whl (30 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Downloading grpc_google_iam_v1-0.14.2-py3-none-any.whl (19 kB)\n",
      "Downloading grpcio_status-1.72.0rc1-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-6.31.0-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Downloading grpcio-1.72.0rc1-cp312-cp312-macosx_11_0_universal2.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shapely, protobuf, packaging, grpcio, google-crc32c, anyio, proto-plus, google-resumable-media, grpcio-status, google-genai, google-api-core, grpc-google-iam-v1, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.71.0\n",
      "    Uninstalling grpcio-1.71.0:\n",
      "      Successfully uninstalled grpcio-1.71.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.7.0\n",
      "    Uninstalling anyio-4.7.0:\n",
      "      Successfully uninstalled anyio-4.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ultralytics 8.3.83 requires ultralytics-thop>=2.0.0, which is not installed.\n",
      "conda 25.1.1 requires conda-libmamba-solver>=24.11.0, but you have conda-libmamba-solver 24.9.0 which is incompatible.\n",
      "opentelemetry-proto 1.33.0 requires protobuf<6.0,>=5.0, but you have protobuf 6.31.0 which is incompatible.\n",
      "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.0 which is incompatible.\n",
      "langchain-core 0.3.60 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anyio-4.9.0 google-api-core-2.24.2 google-cloud-aiplatform-1.93.1 google-cloud-bigquery-3.33.0 google-cloud-core-2.4.3 google-cloud-resource-manager-1.14.2 google-cloud-storage-2.19.0 google-crc32c-1.7.1 google-genai-1.16.1 google-resumable-media-2.7.2 grpc-google-iam-v1-0.14.2 grpcio-1.72.0rc1 grpcio-status-1.72.0rc1 packaging-25.0 proto-plus-1.26.1 protobuf-6.31.0 shapely-2.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9db29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini response: {'parts': [{'text': 'AI learns from data to make predictions or decisions.\\n'}], 'role': 'model'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with your actual Gemini API key\n",
    "API_KEY = \"\n",
    "\n",
    "url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Example prompt for Gemini\n",
    "data = {\n",
    "    \"contents\": [\n",
    "        {\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"text\": \"Explain how AI works in a few words\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    # The generated text is usually in 'candidates' inside 'generateTextResponse'\n",
    "    generated_text = result.get('candidates', [{}])[0].get('content', '')\n",
    "    print(\"Gemini response:\", generated_text)\n",
    "else:\n",
    "    print(f\"Request failed with status {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9141c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: what is lab of future? and what do they do?\n",
      "Retrieved contexts:\n",
      "1. of cutting-edge technology and the spirit of innovation here is truly remarkable. It's an honor to witness the future taking shape, as I'm reminded that the pursuit of excellence knows no bounds, whet...\n",
      "2. of cutting-edge technology and the spirit of innovation here is truly remarkable. It's an honor to witness the future taking shape, as I'm reminded that the pursuit of excellence knows no bounds, whet...\n",
      "3. of cutting-edge technology and the spirit of innovation here is truly remarkable. It's an honor to witness the future taking shape, as I'm reminded that the pursuit of excellence knows no bounds, whet...\n",
      "\n",
      "Gemini answer:\n",
      "{'parts': [{'text': 'Lab of Future is an outstanding facility for developing research across different fields of technology. It is an initiative to transform the scientific temperament of the students and give them global opportunities in the field of Space.\\n'}], 'role': 'model'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "# === Gemini API setup ===\n",
    "API_KEY = \"\"\n",
    "GEMINI_API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}\"\n",
    "\n",
    "def call_gemini_api(prompt_text):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [\n",
    "                    {\"text\": prompt_text}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        generated_text = result.get('candidates', [{}])[0].get('content', '')\n",
    "        return generated_text\n",
    "    else:\n",
    "        raise Exception(f\"Gemini API request failed: {response.status_code} {response.text}\")\n",
    "\n",
    "# === Initialize search system ===\n",
    "def initialize_search_system():\n",
    "    chunked_df = pd.read_csv('laboffuture_content_chunks_metadata.csv')\n",
    "    embeddings = np.load('laboffuture_embeddings.npy')\n",
    "    chunk_texts = chunked_df['chunk_text'].tolist()\n",
    "    urls = chunked_df['url'].tolist()\n",
    "    \n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return model, index, chunk_texts, urls\n",
    "\n",
    "# === Semantic search ===\n",
    "def search_similar_chunks(query_text, model, index, chunk_texts, urls, top_k=3):\n",
    "    query_embedding = model.encode([query_text])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\n",
    "            'chunk_text': chunk_texts[idx],\n",
    "            'url': urls[idx],\n",
    "            'distance': dist\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# === Generate answer with Gemini ===\n",
    "def generate_answer_with_gemini(query, contexts):\n",
    "    prompt = \"Answer the question based on the context below. If you don't know, say 'I don't know'.\\n\\n\"\n",
    "    prompt += \"Context:\\n\"\n",
    "    for i, context in enumerate(contexts):\n",
    "        prompt += f\"{i+1}. {context}\\n\"\n",
    "    prompt += f\"\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    return call_gemini_api(prompt)\n",
    "\n",
    "# === Full RAG flow example ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize retrieval system\n",
    "    model, index, chunk_texts, urls = initialize_search_system()\n",
    "    \n",
    "    # User query\n",
    "    query = \"what is lab of future? and what do they do?\"\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    results = search_similar_chunks(query, model, index, chunk_texts, urls, top_k=3)\n",
    "    contexts = [res['chunk_text'] for res in results]\n",
    "    \n",
    "    # Generate answer using Gemini\n",
    "    answer = generate_answer_with_gemini(query, contexts)\n",
    "    \n",
    "    print(\"User query:\", query)\n",
    "    print(\"Retrieved contexts:\")\n",
    "    for i, ctx in enumerate(contexts, 1):\n",
    "        print(f\"{i}. {ctx[:200]}...\")  # Print snippet\n",
    "    \n",
    "    print(\"\\nGemini answer:\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03da946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: What course they provide ?\n",
      "Retrieved contexts:\n",
      "1. and tools will be provided by the Lab of Future. Personal items may be needed for specific activities. This course includes 60 hours of professional training in the lab (5 levels of 12 hours each) 365...\n",
      "2. and tools will be provided by the Lab of Future. Personal items may be needed for specific activities. This course includes 60 hours of professional training in the lab (5 levels of 12 hours each) 365...\n",
      "3. and tools will be provided by the Lab of Future. Personal items may be needed for specific activities. This course includes 60 hours of professional training in the lab (5 levels of 12 hours each) 365...\n",
      "\n",
      "Gemini answer:\n",
      "The course includes:\n",
      "\n",
      "*   60 hours of professional training in the lab (5 levels of 12 hours each)\n",
      "*   365 days access of learning resources in LMS\n",
      "*   Gold Sealed Certificate upon completion\n",
      "*   Detailed Performance & Skills Report\n",
      "\n",
      "Skills you will learn:\n",
      "\n",
      "1.  Hands-on engineering experience\n",
      "2.  Creative problem-solving\n",
      "3.  Observational analysis\n",
      "4.  Precision and fine motor skills\n",
      "5.  Collaborative teamwork\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from agno.agent import Agent\n",
    "from agno.models.google import Gemini\n",
    "from textwrap import dedent\n",
    "import os\n",
    "\n",
    "# Set your Gemini API key securely\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "# Initialize Gemini model with instructions\n",
    "gemini_model = Gemini(\n",
    "    id=\"gemini-2.0-flash-001\",\n",
    "    instructions=[\n",
    "        \"You are an AI assistant for Lab of Future. Answer questions concisely based on given context.\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "agent = Agent(model=gemini_model, markdown=True)\n",
    "\n",
    "# Initialize search system (same as before)\n",
    "def initialize_search_system():\n",
    "    chunked_df = pd.read_csv('laboffuture_content_chunks_metadata.csv')\n",
    "    embeddings = np.load('laboffuture_embeddings.npy')\n",
    "    chunk_texts = chunked_df['chunk_text'].tolist()\n",
    "    urls = chunked_df['url'].tolist()\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return model, index, chunk_texts, urls\n",
    "\n",
    "# Semantic search (same as before)\n",
    "def search_similar_chunks(query_text, model, index, chunk_texts, urls, top_k=3):\n",
    "    query_embedding = model.encode([query_text])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\n",
    "            'chunk_text': chunk_texts[idx],\n",
    "            'url': urls[idx],\n",
    "            'distance': dist\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Async function to generate answer with Gemini Agent\n",
    "async def generate_answer_with_agent(query, contexts):\n",
    "    prompt = dedent(\"\"\"\\\n",
    "        Context:\n",
    "    \"\"\")\n",
    "    for i, context in enumerate(contexts, 1):\n",
    "        prompt += f\"{i}. {context}\\n\"\n",
    "    prompt += f\"\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    run_response = await agent.arun(prompt)\n",
    "    return run_response.content\n",
    "\n",
    "# Full async chatbot flow\n",
    "async def main():\n",
    "    model, index, chunk_texts, urls = initialize_search_system()\n",
    "    user_query = \"What course they provide ?\"\n",
    "    results = search_similar_chunks(user_query, model, index, chunk_texts, urls, top_k=3)\n",
    "    contexts = [res['chunk_text'] for res in results]\n",
    "\n",
    "    answer = await generate_answer_with_agent(user_query, contexts)\n",
    "\n",
    "    print(\"User query:\", user_query)\n",
    "    print(\"Retrieved contexts:\")\n",
    "    for i, ctx in enumerate(contexts, 1):\n",
    "        print(f\"{i}. {ctx[:200]}...\")  # snippet\n",
    "    print(\"\\nGemini answer:\")\n",
    "    print(answer)\n",
    "\n",
    "# Run the async main function\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fe484",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Lab of Future AI Assistant!\n",
      "Type your question and press Enter. Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tz/kl40j4md6vb32dmh52k53h8m0000gn/T/ipykernel_23768/705857383.py:94: RuntimeWarning: coroutine 'chat_once' was never awaited\n",
      "  print(\"Please try again.\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: asyncio.run() cannot be called from a running event loop\n",
      "Please try again.\n",
      "Error: asyncio.run() cannot be called from a running event loop\n",
      "Please try again.\n",
      "Error: asyncio.run() cannot be called from a running event loop\n",
      "Please try again.\n",
      "Error: asyncio.run() cannot be called from a running event loop\n",
      "Please try again.\n",
      "Error: asyncio.run() cannot be called from a running event loop\n",
      "Please try again.\n",
      "Error: asyncio.run() cannot be called from a running event loop\n",
      "Please try again.\n",
      "Error: asyncio.run() cannot be called from a running event loop\n",
      "Please try again.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Uncomment below line to run the chatbot interactively\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m model, index, chunk_texts, urls \u001b[38;5;241m=\u001b[39m initialize_search_system()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1267\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from agno.agent import Agent\n",
    "from agno.models.google import Gemini\n",
    "from textwrap import dedent, fill\n",
    "import os\n",
    "\n",
    "# Set your Gemini API key securely (replace with your real key)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "# Initialize Gemini agent with instructions\n",
    "gemini_model = Gemini(\n",
    "    id=\"gemini-2.0-flash-001\",\n",
    "    instructions=[\n",
    "        \"You are an AI assistant for Lab of Future. Answer questions concisely based on given context.\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "agent = Agent(model=gemini_model, markdown=True)\n",
    "\n",
    "# Initialize search system (run once)\n",
    "def initialize_search_system():\n",
    "    chunked_df = pd.read_csv('laboffuture_content_chunks_metadata.csv')\n",
    "    embeddings = np.load('laboffuture_embeddings.npy')\n",
    "    chunk_texts = chunked_df['chunk_text'].tolist()\n",
    "    urls = chunked_df['url'].tolist()\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return model, index, chunk_texts, urls\n",
    "\n",
    "# Semantic search function\n",
    "def search_similar_chunks(query_text, model, index, chunk_texts, urls, top_k=3):\n",
    "    query_embedding = model.encode([query_text])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\n",
    "            'chunk_text': chunk_texts[idx],\n",
    "            'url': urls[idx],\n",
    "            'distance': dist\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Async function to generate answer from Gemini agent\n",
    "async def generate_answer_with_agent(query, contexts):\n",
    "    prompt = dedent(\"\"\"\\\n",
    "        Context:\n",
    "    \"\"\")\n",
    "    for i, context in enumerate(contexts, 1):\n",
    "        prompt += f\"{i}. {context}\\n\"\n",
    "    prompt += f\"\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    run_response = await agent.arun(prompt)\n",
    "    return run_response.content\n",
    "\n",
    "# Async chat for a single query\n",
    "async def chat_once(query, model, index, chunk_texts, urls):\n",
    "    results = search_similar_chunks(query, model, index, chunk_texts, urls, top_k=3)\n",
    "    contexts = [res['chunk_text'] for res in results]\n",
    "    answer = await generate_answer_with_agent(query, contexts)\n",
    "    return contexts, answer\n",
    "\n",
    "# Main interactive loop\n",
    "def main():\n",
    "    print(\"Welcome to the Lab of Future AI Assistant!\")\n",
    "    print(\"Type your question and press Enter. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    model, index, chunk_texts, urls = initialize_search_system()\n",
    "\n",
    "    while True:\n",
    "        query = input(\"You: \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        try:\n",
    "            # Run async Gemini + retrieval call synchronously\n",
    "            contexts, answer = asyncio.run(chat_once(query, model, index, chunk_texts, urls))\n",
    "            \n",
    "            print(\"\\nRetrieved contexts:\")\n",
    "            for i, ctx in enumerate(contexts, 1):\n",
    "                print(f\"{i}. {ctx[:200]}...\")  # show snippet\n",
    "            \n",
    "            print(\"\\nAI answer:\\n\")\n",
    "            print(fill(answer, width=80))\n",
    "            print(\"-\" * 80)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Please try again.\")\n",
    "\n",
    "# Uncomment below line to run the chatbot interactively\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41281a55",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Lab of Future AI Assistant!\n",
      "Type your question and press Enter. Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# If you're running this in a notebook or interactive shell, call the main function with `await`\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "Cell \u001b[0;32mIn[1], line 83\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Run async Gemini + retrieval call synchronously using await\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     contexts, answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_once(query, model, index, chunk_texts, urls)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRetrieved contexts:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, ctx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(contexts, \u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[0;32mIn[1], line 66\u001b[0m, in \u001b[0;36mchat_once\u001b[0;34m(query, model, index, chunk_texts, urls)\u001b[0m\n\u001b[1;32m     64\u001b[0m results \u001b[38;5;241m=\u001b[39m search_similar_chunks(query, model, index, chunk_texts, urls, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     65\u001b[0m contexts \u001b[38;5;241m=\u001b[39m [res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[0;32m---> 66\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_answer_with_agent(query, contexts)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m contexts, answer\n",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m, in \u001b[0;36mgenerate_answer_with_agent\u001b[0;34m(query, contexts)\u001b[0m\n\u001b[1;32m     56\u001b[0m     prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 59\u001b[0m run_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39marun(prompt)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/agno/agent/agent.py:1923\u001b[0m, in \u001b[0;36mAgent.arun\u001b[0;34m(self, message, stream, user_id, session_id, audio, images, videos, files, messages, stream_intermediate_steps, retries, knowledge_filters, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m resp\n\u001b[1;32m   1922\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1923\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arun(\n\u001b[1;32m   1924\u001b[0m                 message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   1925\u001b[0m                 stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1926\u001b[0m                 user_id\u001b[38;5;241m=\u001b[39muser_id,\n\u001b[1;32m   1927\u001b[0m                 session_id\u001b[38;5;241m=\u001b[39msession_id,\n\u001b[1;32m   1928\u001b[0m                 audio\u001b[38;5;241m=\u001b[39maudio,\n\u001b[1;32m   1929\u001b[0m                 images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[1;32m   1930\u001b[0m                 videos\u001b[38;5;241m=\u001b[39mvideos,\n\u001b[1;32m   1931\u001b[0m                 files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m   1932\u001b[0m                 messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1933\u001b[0m                 stream_intermediate_steps\u001b[38;5;241m=\u001b[39mstream_intermediate_steps,\n\u001b[1;32m   1934\u001b[0m                 knowledge_filters\u001b[38;5;241m=\u001b[39meffective_filters,\n\u001b[1;32m   1935\u001b[0m                 run_response\u001b[38;5;241m=\u001b[39mrun_response,\n\u001b[1;32m   1936\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1937\u001b[0m             )\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ModelProviderError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1939\u001b[0m     log_warning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_attempts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/agno/agent/agent.py:1566\u001b[0m, in \u001b[0;36mAgent._arun\u001b[0;34m(self, message, stream, session_id, user_id, audio, images, videos, files, messages, stream_intermediate_steps, knowledge_filters, run_response, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_run_response(\n\u001b[1;32m   1559\u001b[0m                     content\u001b[38;5;241m=\u001b[39mmodel_response_chunk\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m   1560\u001b[0m                     event\u001b[38;5;241m=\u001b[39mRunEvent\u001b[38;5;241m.\u001b[39mtool_call_completed,\n\u001b[1;32m   1561\u001b[0m                     run_response\u001b[38;5;241m=\u001b[39mrun_response,\n\u001b[1;32m   1562\u001b[0m                     session_id\u001b[38;5;241m=\u001b[39msession_id,\n\u001b[1;32m   1563\u001b[0m                 )\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;66;03m# Get the model response\u001b[39;00m\n\u001b[0;32m-> 1566\u001b[0m     model_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39maresponse(\n\u001b[1;32m   1567\u001b[0m         messages\u001b[38;5;241m=\u001b[39mrun_messages\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m   1568\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m   1569\u001b[0m         tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tools_for_model,\n\u001b[1;32m   1570\u001b[0m         functions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functions_for_model,\n\u001b[1;32m   1571\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtool_choice,\n\u001b[1;32m   1572\u001b[0m         tool_call_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtool_call_limit,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;66;03m# Format tool calls if they exist\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_response\u001b[38;5;241m.\u001b[39mtool_calls:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/agno/models/base.py:229\u001b[0m, in \u001b[0;36mModel.aresponse\u001b[0;34m(self, messages, response_format, tools, functions, tool_choice, tool_call_limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m model_response \u001b[38;5;241m=\u001b[39m ModelResponse()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Get response from model\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     assistant_message, has_tool_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aprocess_model_response(\n\u001b[1;32m    230\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    231\u001b[0m         model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[1;32m    232\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    233\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    234\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tool_choice,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Handle tool calls if present\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_tool_calls:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# Prepare function calls\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/agno/models/base.py:371\u001b[0m, in \u001b[0;36mModel._aprocess_model_response\u001b[0;34m(self, messages, model_response, response_format, tools, tool_choice)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[1;32m    370\u001b[0m assistant_message\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mstart_timer()\n\u001b[0;32m--> 371\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m    372\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    373\u001b[0m     response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    374\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    375\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39mtool_choice \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tool_choice,\n\u001b[1;32m    376\u001b[0m )\n\u001b[1;32m    377\u001b[0m assistant_message\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mstop_timer()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Parse provider response\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/agno/models/google/gemini.py:295\u001b[0m, in \u001b[0;36mGemini.ainvoke\u001b[0;34m(self, messages, response_format, tools, tool_choice)\u001b[0m\n\u001b[1;32m    292\u001b[0m request_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_kwargs(system_message, response_format\u001b[38;5;241m=\u001b[39mresponse_format, tools\u001b[38;5;241m=\u001b[39mtools)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_client()\u001b[38;5;241m.\u001b[39maio\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m    296\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    297\u001b[0m         contents\u001b[38;5;241m=\u001b[39mformatted_messages,\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_kwargs,\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ClientError, ServerError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    301\u001b[0m     log_error(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError from Gemini API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/genai/models.py:7124\u001b[0m, in \u001b[0;36mAsyncModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   7122\u001b[0m response \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mGenerateContentResponse()\n\u001b[1;32m   7123\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 7124\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content(\n\u001b[1;32m   7125\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel, contents\u001b[38;5;241m=\u001b[39mcontents, config\u001b[38;5;241m=\u001b[39mparsed_config\n\u001b[1;32m   7126\u001b[0m   )\n\u001b[1;32m   7127\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   7128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/genai/models.py:6118\u001b[0m, in \u001b[0;36mAsyncModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   6115\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   6116\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 6118\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39masync_request(\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, path, request_dict, http_options\n\u001b[1;32m   6120\u001b[0m )\n\u001b[1;32m   6122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[1;32m   6123\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[1;32m   6124\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[1;32m   6125\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/genai/_api_client.py:789\u001b[0m, in \u001b[0;36mBaseApiClient.async_request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_request\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[BaseResponse, Any]:\n\u001b[1;32m    785\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    786\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m    787\u001b[0m   )\n\u001b[0;32m--> 789\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_request(http_request\u001b[38;5;241m=\u001b[39mhttp_request, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    790\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mjson\n\u001b[1;32m    791\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/genai/_api_client.py:726\u001b[0m, in \u001b[0;36mBaseApiClient._async_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    723\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    724\u001b[0m   )\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 726\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    727\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    728\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    729\u001b[0m       headers\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    730\u001b[0m       content\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    731\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    732\u001b[0m   )\n\u001b[1;32m    733\u001b[0m   \u001b[38;5;28;01mawait\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mAPIError\u001b[38;5;241m.\u001b[39mraise_for_async_response(response)\n\u001b[1;32m    734\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    735\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    736\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:1540\u001b[0m, in \u001b[0;36mAsyncClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1527\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m   1528\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m   1529\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m   1539\u001b[0m )\n\u001b[0;32m-> 1540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m   1627\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1629\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1630\u001b[0m     request,\n\u001b[1;32m   1631\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1632\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1633\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1634\u001b[0m )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1654\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auth_flow\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1657\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1658\u001b[0m         request,\n\u001b[1;32m   1659\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1660\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1661\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1694\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1727\u001b[0m     )\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1730\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1733\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    381\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    382\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    383\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    392\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 394\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    399\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    400\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    401\u001b[0m     stream\u001b[38;5;241m=\u001b[39mAsyncResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    402\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    403\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_async/connection.py:101\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_async/connection.py:78\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_lock:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     80\u001b[0m         ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m         http2_negotiated \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     82\u001b[0m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object\u001b[38;5;241m.\u001b[39mselected_alpn_protocol() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_async/connection.py:124\u001b[0m, in \u001b[0;36mAsyncHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mport,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket_options,\n\u001b[1;32m    122\u001b[0m     }\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    125\u001b[0m         trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_backends/auto.py:31\u001b[0m, in \u001b[0;36mAutoBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect_tcp\u001b[39m(\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     24\u001b[0m     host: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     socket_options: typing\u001b[38;5;241m.\u001b[39mIterable[SOCKET_OPTION] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncNetworkStream:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_backend()\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\n\u001b[1;32m     32\u001b[0m         host,\n\u001b[1;32m     33\u001b[0m         port,\n\u001b[1;32m     34\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     35\u001b[0m         local_address\u001b[38;5;241m=\u001b[39mlocal_address,\n\u001b[1;32m     36\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39msocket_options,\n\u001b[1;32m     37\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpcore/_backends/anyio.py:115\u001b[0m, in \u001b[0;36mAnyIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n\u001b[0;32m--> 115\u001b[0m         stream: anyio\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mByteStream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mconnect_tcp(\n\u001b[1;32m    116\u001b[0m             remote_host\u001b[38;5;241m=\u001b[39mhost,\n\u001b[1;32m    117\u001b[0m             remote_port\u001b[38;5;241m=\u001b[39mport,\n\u001b[1;32m    118\u001b[0m             local_host\u001b[38;5;241m=\u001b[39mlocal_address,\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;66;03m# By default TCP sockets opened in `asyncio` include TCP_NODELAY.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/anyio/_core/_sockets.py:208\u001b[0m, in \u001b[0;36mconnect_tcp\u001b[0;34m(remote_host, remote_port, local_host, tls, ssl_context, tls_standard_compatible, tls_hostname, happy_eyeballs_delay)\u001b[0m\n\u001b[1;32m    205\u001b[0m         target_addrs \u001b[38;5;241m=\u001b[39m [(socket\u001b[38;5;241m.\u001b[39mAF_INET, addr_obj\u001b[38;5;241m.\u001b[39mcompressed)]\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# getaddrinfo() will raise an exception if name resolution fails\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     gai_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m getaddrinfo(\n\u001b[1;32m    209\u001b[0m         target_host, remote_port, family\u001b[38;5;241m=\u001b[39mfamily, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39msocket\u001b[38;5;241m.\u001b[39mSOCK_STREAM\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# Organize the list so that the first address is an IPv6 address (if available)\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# and the second one is an IPv4 addresses. The rest can be in whatever order.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     v6_found \u001b[38;5;241m=\u001b[39m v4_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/anyio/_core/_sockets.py:584\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     encoded_host \u001b[38;5;241m=\u001b[39m host\n\u001b[0;32m--> 584\u001b[0m gai_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend()\u001b[38;5;241m.\u001b[39mgetaddrinfo(\n\u001b[1;32m    585\u001b[0m     encoded_host, port, family\u001b[38;5;241m=\u001b[39mfamily, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m, proto\u001b[38;5;241m=\u001b[39mproto, flags\u001b[38;5;241m=\u001b[39mflags\n\u001b[1;32m    586\u001b[0m )\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    588\u001b[0m     (family, \u001b[38;5;28mtype\u001b[39m, proto, canonname, convert_ipv6_sockaddr(sockaddr))\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m family, \u001b[38;5;28mtype\u001b[39m, proto, canonname, sockaddr \u001b[38;5;129;01min\u001b[39;00m gai_res\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# filter out IPv6 results when IPv6 is disabled\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sockaddr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    592\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:2692\u001b[0m, in \u001b[0;36mAsyncIOBackend.getaddrinfo\u001b[0;34m(cls, host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2674\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetaddrinfo\u001b[39m(\n\u001b[1;32m   2675\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2690\u001b[0m     ]\n\u001b[1;32m   2691\u001b[0m ]:\n\u001b[0;32m-> 2692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_running_loop()\u001b[38;5;241m.\u001b[39mgetaddrinfo(\n\u001b[1;32m   2693\u001b[0m         host, port, family\u001b[38;5;241m=\u001b[39mfamily, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m, proto\u001b[38;5;241m=\u001b[39mproto, flags\u001b[38;5;241m=\u001b[39mflags\n\u001b[1;32m   2694\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/asyncio/base_events.py:899\u001b[0m, in \u001b[0;36mBaseEventLoop.getaddrinfo\u001b[0;34m(self, host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    897\u001b[0m     getaddr_func \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo\n\u001b[0;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m, getaddr_func, host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from agno.agent import Agent\n",
    "from agno.models.google import Gemini\n",
    "from textwrap import dedent, fill\n",
    "import os\n",
    "\n",
    "# Set your Gemini API key securely (replace with your real key)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "# Initialize Gemini agent with instructions\n",
    "gemini_model = Gemini(\n",
    "    id=\"gemini-2.0-flash-001\",\n",
    "    instructions=[\n",
    "        \"You are an AI assistant for Lab of Future. Answer questions concisely based on given context.\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "agent = Agent(model=gemini_model, markdown=True)\n",
    "\n",
    "# Initialize search system (run once)\n",
    "def initialize_search_system():\n",
    "    chunked_df = pd.read_csv('laboffuture_content_chunks_metadata.csv')\n",
    "    embeddings = np.load('laboffuture_embeddings.npy')\n",
    "    chunk_texts = chunked_df['chunk_text'].tolist()\n",
    "    urls = chunked_df['url'].tolist()\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return model, index, chunk_texts, urls\n",
    "\n",
    "# Semantic search function\n",
    "def search_similar_chunks(query_text, model, index, chunk_texts, urls, top_k=3):\n",
    "    query_embedding = model.encode([query_text])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\n",
    "            'chunk_text': chunk_texts[idx],\n",
    "            'url': urls[idx],\n",
    "            'distance': dist\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Async function to generate answer from Gemini agent\n",
    "async def generate_answer_with_agent(query, contexts):\n",
    "    prompt = dedent(\"\"\"\\\n",
    "        Context:\n",
    "    \"\"\")\n",
    "    for i, context in enumerate(contexts, 1):\n",
    "        prompt += f\"{i}. {context}\\n\"\n",
    "    prompt += f\"\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    run_response = await agent.arun(prompt)\n",
    "    return run_response.content\n",
    "\n",
    "# Async chat for a single query\n",
    "async def chat_once(query, model, index, chunk_texts, urls):\n",
    "    results = search_similar_chunks(query, model, index, chunk_texts, urls, top_k=3)\n",
    "    contexts = [res['chunk_text'] for res in results]\n",
    "    answer = await generate_answer_with_agent(query, contexts)\n",
    "    return contexts, answer\n",
    "\n",
    "# Main interactive loop\n",
    "async def main():\n",
    "    print(\"Welcome to the Lab of Future AI Assistant!\")\n",
    "    print(\"Type your question and press Enter. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    model, index, chunk_texts, urls = initialize_search_system()\n",
    "\n",
    "    while True:\n",
    "        query = input(\"You: \").strip()\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        try:\n",
    "            # Run async Gemini + retrieval call synchronously using await\n",
    "            contexts, answer = await chat_once(query, model, index, chunk_texts, urls)\n",
    "\n",
    "            print(\"\\nRetrieved contexts:\")\n",
    "            for i, ctx in enumerate(contexts, 1):\n",
    "                print(f\"{i}. {ctx[:200]}...\")  # show snippet\n",
    "\n",
    "            print(\"\\nAI answer:\\n\")\n",
    "            print(fill(answer, width=80))\n",
    "            print(\"-\" * 80)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Please try again.\")\n",
    "\n",
    "# If you're running this in a notebook or interactive shell, call the main function with `await`\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
